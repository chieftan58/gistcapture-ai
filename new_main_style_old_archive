# main.py - Renaissance Weekly Podcast Intelligence System (Production Version - Fixed)
import os
import json
import hashlib
import asyncio
import aiohttp
import aiofiles
import tempfile
import feedparser
from datetime import datetime, timedelta
from pathlib import Path
from urllib.parse import urlparse, urljoin
import openai
from openai import OpenAI
from pydub import AudioSegment
from sendgrid import SendGridAPIClient
from sendgrid.helpers.mail import Mail
from dotenv import load_dotenv
import time
import re
import threading
from typing import List, Dict, Optional, Tuple
import requests
from bs4 import BeautifulSoup
import html
import webbrowser
from http.server import HTTPServer, SimpleHTTPRequestHandler
from dataclasses import dataclass
from enum import Enum
import sqlite3
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

# Load environment variables
load_dotenv()

# Initialize clients with improved settings
openai_client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    timeout=300.0,  # 5 minute timeout for long transcriptions
    max_retries=2   # Retry failed requests
)
sendgrid_client = SendGridAPIClient(os.getenv("SENDGRID_API_KEY"))

# Configuration
TRANSCRIPT_DIR = Path("transcripts")
AUDIO_DIR = Path("audio")
SUMMARY_DIR = Path("summaries")
CACHE_DIR = Path("cache")
TEMP_DIR = Path("temp")
DB_PATH = Path("podcast_data.db")

# Create directories
for dir in [TRANSCRIPT_DIR, AUDIO_DIR, SUMMARY_DIR, CACHE_DIR, TEMP_DIR]:
    dir.mkdir(exist_ok=True)

# Email configuration
EMAIL_FROM = "insights@gistcapture.ai"
EMAIL_TO = os.getenv("EMAIL_TO", "caddington05@gmail.com")

# Logging setup with UTF-8 encoding for Windows compatibility
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('renaissance_weekly.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Testing mode
TESTING_MODE = os.getenv("TESTING_MODE", "true").lower() == "true"
MAX_TRANSCRIPTION_MINUTES = 20 if TESTING_MODE else float('inf')


class TranscriptSource(Enum):
    OFFICIAL = "official"          # From podcast website
    API = "api"                    # From transcript API
    COMMUNITY = "community"        # From community sources
    GENERATED = "generated"        # We transcribed it
    CACHED = "cached"             # From our cache


@dataclass
class Episode:
    podcast: str
    title: str
    published: datetime
    audio_url: Optional[str] = None
    transcript_url: Optional[str] = None
    transcript_source: Optional[TranscriptSource] = None
    description: str = ""
    link: str = ""
    duration: str = "Unknown"
    guid: str = ""  # Unique identifier
    
    def __post_init__(self):
        if not self.guid:
            # Create unique ID from podcast + title + date
            self.guid = hashlib.md5(
                f"{self.podcast}:{self.title}:{self.published}".encode()
            ).hexdigest()


class PodcastDatabase:
    """SQLite database for tracking podcasts and episodes"""
    
    def __init__(self, db_path: Path):
        self.db_path = db_path
        self._init_db()
    
    def _init_db(self):
        """Initialize database schema"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Podcast feeds table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS podcast_feeds (
                podcast_name TEXT PRIMARY KEY,
                working_feeds TEXT,  -- JSON array of working feed URLs
                transcript_sources TEXT,  -- JSON object of transcript sources
                last_checked TIMESTAMP,
                last_success TIMESTAMP
            )
        """)
        
        # Episodes table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS episodes (
                guid TEXT PRIMARY KEY,
                podcast_name TEXT,
                title TEXT,
                published TIMESTAMP,
                audio_url TEXT,
                transcript_url TEXT,
                transcript_source TEXT,
                transcript_text TEXT,
                summary TEXT,
                processed BOOLEAN DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (podcast_name) REFERENCES podcast_feeds(podcast_name)
            )
        """)
        
        # Feed health monitoring
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS feed_health (
                url TEXT PRIMARY KEY,
                podcast_name TEXT,
                status TEXT,  -- 'working', 'failing', 'dead'
                last_success TIMESTAMP,
                last_failure TIMESTAMP,
                failure_count INTEGER DEFAULT 0,
                error_message TEXT
            )
        """)
        
        conn.commit()
        conn.close()
    
    def get_cached_transcript(self, episode: Episode) -> Optional[str]:
        """Get cached transcript if available"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute(
            "SELECT transcript_text FROM episodes WHERE guid = ?",
            (episode.guid,)
        )
        result = cursor.fetchone()
        conn.close()
        
        return result[0] if result and result[0] else None
    
    def save_episode(self, episode: Episode, transcript: Optional[str] = None):
        """Save episode to database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT OR REPLACE INTO episodes 
            (guid, podcast_name, title, published, audio_url, transcript_url, 
             transcript_source, transcript_text)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            episode.guid,
            episode.podcast,
            episode.title,
            episode.published,
            episode.audio_url,
            episode.transcript_url,
            episode.transcript_source.value if episode.transcript_source else None,
            transcript
        ))
        
        conn.commit()
        conn.close()


class TranscriptFinder:
    """Find existing transcripts before resorting to audio transcription"""
    
    def __init__(self, db: PodcastDatabase):
        self.db = db
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Renaissance Weekly/2.0 (Transcript Finder)'
        })
    
    async def find_transcript(self, episode: Episode) -> Tuple[Optional[str], Optional[TranscriptSource]]:
        """Try to find an existing transcript"""
        logger.info(f"🔍 Searching for transcript: {episode.title}")
        
        # 1. Check cache first
        cached = self.db.get_cached_transcript(episode)
        if cached:
            logger.info("✅ Found cached transcript")
            return cached, TranscriptSource.CACHED
        
        # 2. Try official sources
        transcript = await self._try_official_transcript(episode)
        if transcript:
            return transcript, TranscriptSource.OFFICIAL
        
        # 3. Try transcript APIs
        transcript = await self._try_transcript_apis(episode)
        if transcript:
            return transcript, TranscriptSource.API
        
        # 4. Try community sources
        transcript = await self._try_community_sources(episode)
        if transcript:
            return transcript, TranscriptSource.COMMUNITY
        
        logger.info("❌ No existing transcript found")
        return None, None
    
    async def _try_official_transcript(self, episode: Episode) -> Optional[str]:
        """Check official podcast websites for transcripts"""
        
        # Podcast-specific transcript patterns
        transcript_patterns = {
            "Tim Ferriss": self._get_tim_ferriss_transcript,
            "Huberman Lab": self._get_huberman_transcript,
            "Lex Fridman": self._get_lex_fridman_transcript,
            "The Drive": self._get_peter_attia_transcript,
            "Modern Wisdom": self._get_modern_wisdom_transcript,
            "Knowledge Project": self._get_knowledge_project_transcript,
        }
        
        if episode.podcast in transcript_patterns:
            try:
                return await transcript_patterns[episode.podcast](episode)
            except Exception as e:
                logger.error(f"Error getting official transcript: {e}")
        
        # Generic transcript finder for episode page
        if episode.link:
            return await self._find_transcript_on_page(episode.link)
        
        return None
    
    async def _get_tim_ferriss_transcript(self, episode: Episode) -> Optional[str]:
        """Tim Ferriss provides full transcripts on his website"""
        try:
            # Tim's transcripts are usually at the episode URL
            if not episode.link:
                return None
            
            response = self.session.get(episode.link, timeout=15)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for transcript section
            transcript_div = soup.find('div', class_='podcast-transcript')
            if not transcript_div:
                # Try another common pattern
                transcript_div = soup.find('div', id='transcript')
            
            if transcript_div:
                # Clean up the transcript
                transcript = transcript_div.get_text(separator='\n', strip=True)
                if len(transcript) > 1000:  # Sanity check
                    logger.info("✅ Found Tim Ferriss transcript")
                    return transcript
        except Exception as e:
            logger.error(f"Error fetching Tim Ferriss transcript: {e}")
        
        return None
    
    async def _get_huberman_transcript(self, episode: Episode) -> Optional[str]:
        """Huberman Lab provides transcripts"""
        try:
            if episode.link and 'hubermanlab.com' in episode.link:
                response = self.session.get(episode.link, timeout=15)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Huberman uses specific markup
                transcript_section = soup.find('section', {'aria-label': 'Transcript'})
                if not transcript_section:
                    transcript_section = soup.find('div', class_='transcript-content')
                
                if transcript_section:
                    transcript = transcript_section.get_text(separator='\n', strip=True)
                    if len(transcript) > 1000:
                        logger.info("✅ Found Huberman Lab transcript")
                        return transcript
        except Exception as e:
            logger.error(f"Error fetching Huberman transcript: {e}")
        
        return None
    
    async def _get_lex_fridman_transcript(self, episode: Episode) -> Optional[str]:
        """Lex Fridman provides transcripts on his website"""
        try:
            # Lex's site structure
            if episode.link and 'lexfridman.com' in episode.link:
                response = self.session.get(episode.link, timeout=15)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Look for transcript
                transcript_div = soup.find('div', class_='transcript')
                if transcript_div:
                    transcript = transcript_div.get_text(separator='\n', strip=True)
                    if len(transcript) > 1000:
                        logger.info("✅ Found Lex Fridman transcript")
                        return transcript
        except Exception as e:
            logger.error(f"Error fetching Lex Fridman transcript: {e}")
        
        return None
    
    async def _get_peter_attia_transcript(self, episode: Episode) -> Optional[str]:
        """Peter Attia's The Drive provides show notes with key quotes"""
        # Note: Full transcripts might be member-only
        # We can get show notes which are often quite detailed
        try:
            if episode.link and 'peterattiamd.com' in episode.link:
                response = self.session.get(episode.link, timeout=15)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Get detailed show notes
                show_notes = soup.find('div', class_='show-notes-content')
                if show_notes:
                    notes = show_notes.get_text(separator='\n', strip=True)
                    if len(notes) > 2000:  # Show notes are substantial
                        logger.info("✅ Found Peter Attia show notes")
                        return f"[Show Notes]\n{notes}"
        except Exception as e:
            logger.error(f"Error fetching Peter Attia content: {e}")
        
        return None
    
    async def _get_modern_wisdom_transcript(self, episode: Episode) -> Optional[str]:
        """Modern Wisdom sometimes provides transcripts"""
        # Similar pattern - check website
        return await self._find_transcript_on_page(episode.link)
    
    async def _get_knowledge_project_transcript(self, episode: Episode) -> Optional[str]:
        """Knowledge Project provides detailed show notes"""
        if episode.link and 'fs.blog' in episode.link:
            return await self._find_transcript_on_page(episode.link)
        return None
    
    async def _find_transcript_on_page(self, url: str) -> Optional[str]:
        """Generic transcript finder for any podcast page"""
        if not url:
            return None
        
        try:
            response = self.session.get(url, timeout=15)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Common transcript indicators
            transcript_indicators = [
                ('div', {'class': re.compile(r'transcript|Transcript')}),
                ('div', {'id': re.compile(r'transcript|Transcript')}),
                ('section', {'class': re.compile(r'transcript|Transcript')}),
                ('article', {'class': re.compile(r'transcript|Transcript')}),
                ('div', {'data-transcript': True}),
            ]
            
            for tag, attrs in transcript_indicators:
                element = soup.find(tag, attrs)
                if element:
                    text = element.get_text(separator='\n', strip=True)
                    if len(text) > 1000:  # Minimum length check
                        logger.info(f"✅ Found transcript on page: {url}")
                        return text
            
            # Check for "Read Transcript" or similar links
            transcript_link = soup.find('a', text=re.compile(r'transcript|Transcript', re.I))
            if transcript_link and transcript_link.get('href'):
                transcript_url = urljoin(url, transcript_link['href'])
                return await self._fetch_transcript_from_url(transcript_url)
        
        except Exception as e:
            logger.error(f"Error finding transcript on page: {e}")
        
        return None
    
    async def _fetch_transcript_from_url(self, url: str) -> Optional[str]:
        """Fetch transcript from a direct URL"""
        try:
            response = self.session.get(url, timeout=15)
            if response.status_code == 200:
                # Could be PDF, TXT, or HTML
                content_type = response.headers.get('content-type', '').lower()
                
                if 'text' in content_type or 'html' in content_type:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    text = soup.get_text(separator='\n', strip=True)
                    if len(text) > 1000:
                        return text
                    
        except Exception as e:
            logger.error(f"Error fetching transcript from URL: {e}")
        
        return None
    
    async def _try_transcript_apis(self, episode: Episode) -> Optional[str]:
        """Try transcript API services"""
        
        # 1. Taddy API (if you have access)
        if os.getenv("TADDY_API_KEY"):
            transcript = await self._try_taddy_api(episode)
            if transcript:
                return transcript
        
        # 2. Rev.ai (many podcasts use Rev)
        # This would require either API access or scraping
        
        # 3. Descript (some podcasts use this)
        # Would need to implement based on their API
        
        return None
    
    async def _try_taddy_api(self, episode: Episode) -> Optional[str]:
        """Taddy provides transcripts for many podcasts"""
        # Implementation would depend on Taddy API access
        # This is a placeholder
        return None
    
    async def _try_community_sources(self, episode: Episode) -> Optional[str]:
        """Check community transcript sources"""
        
        # 1. GitHub repositories with transcripts
        github_sources = [
            "https://github.com/leerob/lex-fridman-transcripts",
            # Add more known transcript repos
        ]
        
        # 2. Fan sites with transcripts
        # Many popular podcasts have fan-maintained transcript sites
        
        # 3. Reddit posts with transcripts
        # Some communities post transcripts
        
        return None


class PodcastIndexClient:
    """Client for PodcastIndex.org - free and comprehensive"""
    
    def __init__(self):
        self.api_key = os.getenv("PODCASTINDEX_API_KEY")
        self.api_secret = os.getenv("PODCASTINDEX_API_SECRET")
        self.base_url = "https://api.podcastindex.org/api/1.0"
    
    def _get_headers(self):
        """Generate auth headers for PodcastIndex"""
        if not self.api_key or not self.api_secret:
            return None
            
        import hashlib
        import time
        
        api_header_time = str(int(time.time()))
        hash_input = self.api_key + self.api_secret + api_header_time
        sha1_hash = hashlib.sha1(hash_input.encode()).hexdigest()
        
        return {
            "X-Auth-Key": self.api_key,
            "X-Auth-Date": api_header_time,
            "Authorization": sha1_hash,
            "User-Agent": "Renaissance Weekly"
        }
    
    async def search_podcast(self, podcast_name: str) -> Optional[Dict]:
        """Search for podcast by name"""
        headers = self._get_headers()
        if not headers:
            return None
            
        try:
            url = f"{self.base_url}/search/byterm"
            params = {"q": podcast_name, "val": "podcast"}
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, params=params) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        if data.get("feeds"):
                            return data["feeds"][0]  # Return first match
        except Exception as e:
            logger.error(f"PodcastIndex search error: {e}")
        
        return None
    
    async def get_episodes(self, feed_id: int, max_results: int = 20) -> List[Dict]:
        """Get recent episodes for a podcast"""
        headers = self._get_headers()
        if not headers:
            return []
            
        try:
            url = f"{self.base_url}/episodes/byfeedid"
            params = {"id": feed_id, "max": max_results}
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, params=params) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        return data.get("items", [])
        except Exception as e:
            logger.error(f"PodcastIndex episodes error: {e}")
        
        return []


class ReliableEpisodeFetcher:
    """Fetches episodes with multiple fallback sources"""
    
    def __init__(self, db: PodcastDatabase):
        self.db = db
        self.podcast_index = PodcastIndexClient()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    async def fetch_episodes(self, podcast_config: Dict, days_back: int = 7) -> List[Episode]:
        """Bulletproof episode fetching - ensures we find ALL episodes"""
        podcast_name = podcast_config["name"]
        logger.info(f"📡 Fetching episodes for {podcast_name}")
        
        episodes = []
        
        # If force_apple is set, go straight to Apple
        if podcast_config.get("force_apple") and "apple_id" in podcast_config:
            logger.info(f"   Force using Apple Podcasts for {podcast_name}")
            episodes = await self._try_apple_podcasts(podcast_name, podcast_config["apple_id"], days_back)
            if episodes:
                logger.info(f"✅ Found {len(episodes)} episodes via Apple Podcasts")
                return episodes
        
        # Method 1: Try RSS feeds
        if "rss_feeds" in podcast_config and not podcast_config.get("force_apple"):
            episodes = await self._try_rss_feeds(podcast_name, podcast_config["rss_feeds"], days_back)
            if episodes:
                logger.info(f"✅ Found {len(episodes)} episodes via RSS")
                return episodes
        
        # Method 2: Try Apple Podcasts as fallback
        if "apple_id" in podcast_config:
            episodes = await self._try_apple_podcasts(podcast_name, podcast_config["apple_id"], days_back)
            if episodes:
                logger.info(f"✅ Found {len(episodes)} episodes via Apple Podcasts")
                return episodes
        
        # Method 3: Try PodcastIndex
        episodes = await self._try_podcast_index(podcast_name, days_back)
        if episodes:
            logger.info(f"✅ Found {len(episodes)} episodes via PodcastIndex")
            return episodes
        
        # Method 4: Try web scraping
        if "website" in podcast_config:
            episodes = await self._try_web_scraping(podcast_name, podcast_config["website"], days_back)
            if episodes:
                logger.info(f"✅ Found {len(episodes)} episodes via web scraping")
                return episodes
        
        # Method 5: Try iTunes Search API as last resort
        episodes = await self._try_itunes_search(podcast_name, days_back)
        if episodes:
            logger.info(f"✅ Found {len(episodes)} episodes via iTunes Search")
            return episodes
        
        logger.error(f"❌ Could not fetch episodes for {podcast_name} from any source")
        return []
    
    async def _try_itunes_search(self, podcast_name: str, days_back: int) -> List[Episode]:
        """Use iTunes Search API to find podcast and get episodes"""
        try:
            # Search for podcast
            search_url = "https://itunes.apple.com/search"
            params = {
                "term": podcast_name,
                "media": "podcast",
                "entity": "podcast",
                "limit": 5
            }
            
            response = self.session.get(search_url, params=params, timeout=10)
            if response.status_code != 200:
                return []
            
            data = response.json()
            if not data.get("results"):
                return []
            
            # Try each result until we find episodes
            for result in data["results"]:
                feed_url = result.get("feedUrl")
                if feed_url:
                    episodes = await self._try_rss_feeds(podcast_name, [feed_url], days_back)
                    if episodes:
                        return episodes
                    
        except Exception as e:
            logger.error(f"iTunes Search error: {e}")
        
        return []
    
    async def fetch_missing_from_apple(self, podcast_config: Dict, existing_episodes: List[Episode], verification_result: Dict) -> List[Episode]:
        """Fetch missing episodes identified by Apple verification"""
        if verification_result["status"] != "success" or verification_result["missing_count"] == 0:
            return []
        
        podcast_name = podcast_config["name"]
        apple_feed_url = verification_result["apple_feed_url"]
        
        logger.info(f"🔄 Attempting to fetch {verification_result['missing_count']} missing episodes from Apple feed")
        
        # Fetch from Apple's feed URL
        additional_episodes = await self._try_rss_feeds(podcast_name, [apple_feed_url], 30)  # Look back further
        
        if not additional_episodes:
            return []
        
        # Filter to only the missing ones
        existing_titles = {ep.title.lower().strip() for ep in existing_episodes}
        existing_guids = {ep.guid for ep in existing_episodes}
        
        new_episodes = []
        for ep in additional_episodes:
            if ep.title.lower().strip() not in existing_titles and ep.guid not in existing_guids:
                new_episodes.append(ep)
        
        if new_episodes:
            logger.info(f"✅ Successfully fetched {len(new_episodes)} missing episodes from Apple feed")
        
        return new_episodes
    
    async def _try_rss_feeds(self, podcast_name: str, feed_urls: List[str], days_back: int) -> List[Episode]:
        """Try multiple RSS feed URLs with better error handling and timeouts"""
        cutoff = datetime.now() - timedelta(days=days_back)
        
        for url in feed_urls:
            try:
                logger.info(f"  Trying RSS: {url}")
                
                # Use requests with shorter timeout for better control
                try:
                    response = self.session.get(url, timeout=10, allow_redirects=True)
                    if response.status_code != 200:
                        logger.warning(f"  HTTP {response.status_code} for {url}")
                        continue
                    feed_content = response.content
                except requests.Timeout:
                    logger.warning(f"  Timeout for {url}")
                    continue
                except Exception as e:
                    logger.warning(f"  Request error: {e}")
                    continue
                
                # Parse the feed content
                feed = feedparser.parse(feed_content)
                
                # Check if feed is valid
                if feed.bozo and feed.bozo_exception:
                    logger.warning(f"  Feed parse warning: {feed.bozo_exception}")
                
                if not feed.entries:
                    logger.warning(f"  No entries found in feed")
                    continue
                
                episodes = []
                for entry in feed.entries[:20]:
                    try:
                        # Parse date
                        pub_date = self._parse_date(entry)
                        if not pub_date:
                            logger.debug(f"  Skipping entry without date: {entry.get('title', 'Unknown')}")
                            continue
                        
                        if pub_date < cutoff:
                            continue
                        
                        # Get audio URL
                        audio_url = self._extract_audio_url(entry)
                        if not audio_url:
                            logger.debug(f"  Skipping entry without audio: {entry.get('title', 'Unknown')}")
                            continue
                        
                        # Check for transcript URL in feed
                        transcript_url = self._extract_transcript_url(entry)
                        
                        # Extract full description
                        description = self._extract_full_description(entry)
                        
                        # Extract and format duration
                        duration = self._extract_duration(entry)
                        
                        episode = Episode(
                            podcast=podcast_name,
                            title=entry.get('title', 'Unknown'),
                            published=pub_date,
                            audio_url=audio_url,
                            transcript_url=transcript_url,
                            description=description,
                            link=entry.get('link', ''),
                            duration=duration,
                            guid=entry.get('guid', entry.get('id', ''))
                        )
                        
                        episodes.append(episode)
                    except Exception as e:
                        logger.warning(f"  Error processing entry: {e}")
                        continue
                
                if episodes:
                    return episodes
                    
            except Exception as e:
                logger.error(f"  RSS error for {url}: {e}")
                continue
        
        return []
    
    async def _try_podcast_index(self, podcast_name: str, days_back: int) -> List[Episode]:
        """Try PodcastIndex.org API"""
        # Search for podcast
        podcast_info = await self.podcast_index.search_podcast(podcast_name)
        if not podcast_info:
            return []
        
        # Get episodes
        feed_id = podcast_info.get("id")
        if not feed_id:
            return []
            
        episodes_data = await self.podcast_index.get_episodes(feed_id)
        
        episodes = []
        cutoff = datetime.now() - timedelta(days=days_back)
        
        for ep_data in episodes_data:
            pub_date = datetime.fromtimestamp(ep_data.get("datePublished", 0))
            if pub_date < cutoff:
                continue
            
            episode = Episode(
                podcast=podcast_name,
                title=ep_data.get("title", "Unknown"),
                published=pub_date,
                audio_url=ep_data.get("enclosureUrl"),
                transcript_url=ep_data.get("transcriptUrl"),  # Some episodes have this
                description=ep_data.get("description", ""),
                link=ep_data.get("link", ""),
                duration=self._seconds_to_duration(ep_data.get("duration", 0)),
                guid=ep_data.get("guid", str(ep_data.get("id", "")))
            )
            
            episodes.append(episode)
        
        return episodes
    
    async def _try_apple_podcasts(self, podcast_name: str, apple_id: str, days_back: int) -> List[Episode]:
        """Use Apple Podcasts lookup to get RSS feed"""
        try:
            lookup_url = f"https://itunes.apple.com/lookup?id={apple_id}&entity=podcast"
            response = self.session.get(lookup_url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                if data.get("results"):
                    feed_url = data["results"][0].get("feedUrl")
                    if feed_url:
                        return await self._try_rss_feeds(podcast_name, [feed_url], days_back)
        except Exception as e:
            logger.error(f"Apple Podcasts error: {e}")
        
        return []
    
    async def verify_against_apple_podcasts(self, podcast_config: Dict, found_episodes: List[Episode], days_back: int) -> Dict:
        """Verify found episodes against Apple Podcasts to check for missing episodes"""
        if "apple_id" not in podcast_config:
            return {"status": "skipped", "reason": "No Apple ID configured"}
        
        try:
            apple_id = podcast_config["apple_id"]
            podcast_name = podcast_config["name"]
            
            # First get the RSS feed URL from Apple
            lookup_url = f"https://itunes.apple.com/lookup?id={apple_id}&entity=podcast"
            response = self.session.get(lookup_url, timeout=10)
            
            if response.status_code != 200:
                return {"status": "error", "reason": f"Apple API returned {response.status_code}"}
            
            data = response.json()
            if not data.get("results"):
                return {"status": "error", "reason": "No podcast found on Apple"}
            
            apple_feed_url = data["results"][0].get("feedUrl")
            if not apple_feed_url:
                return {"status": "error", "reason": "No feed URL in Apple data"}
            
            # Parse Apple's feed
            apple_feed = feedparser.parse(apple_feed_url, agent='Mozilla/5.0')
            
            if not apple_feed.entries:
                return {"status": "error", "reason": "No episodes in Apple feed"}
            
            # Get episodes from Apple feed within date range
            cutoff = datetime.now() - timedelta(days=days_back)
            apple_episodes = []
            
            for entry in apple_feed.entries[:20]:  # Check recent 20 episodes
                pub_date = self._parse_date(entry)
                if pub_date and pub_date >= cutoff:
                    title = entry.get('title', 'Unknown')
                    apple_episodes.append({
                        'title': title,
                        'date': pub_date,
                        'guid': entry.get('guid', entry.get('id', '')),
                        'has_audio': bool(self._extract_audio_url(entry))
                    })
            
            # Compare with found episodes
            found_titles = {ep.title.lower().strip() for ep in found_episodes}
            found_guids = {ep.guid for ep in found_episodes}
            
            missing_episodes = []
            for apple_ep in apple_episodes:
                # Check by title (normalized) and GUID
                title_normalized = apple_ep['title'].lower().strip()
                if title_normalized not in found_titles and apple_ep['guid'] not in found_guids:
                    missing_episodes.append(apple_ep)
            
            result = {
                "status": "success",
                "apple_episode_count": len(apple_episodes),
                "found_episode_count": len([ep for ep in found_episodes if ep.published >= cutoff]),
                "missing_count": len(missing_episodes),
                "missing_episodes": missing_episodes,
                "apple_feed_url": apple_feed_url
            }
            
            if missing_episodes:
                logger.warning(f"⚠️  {podcast_name}: Found {len(missing_episodes)} episodes on Apple Podcasts that we missed:")
                for ep in missing_episodes[:3]:  # Show first 3
                    logger.warning(f"   - {ep['title']} ({ep['date'].strftime('%Y-%m-%d')})")
                if len(missing_episodes) > 3:
                    logger.warning(f"   ... and {len(missing_episodes) - 3} more")
            
            return result
            
        except Exception as e:
            logger.error(f"Apple verification error for {podcast_config['name']}: {e}")
            return {"status": "error", "reason": str(e)}
    
    async def _try_web_scraping(self, podcast_name: str, website: str, days_back: int) -> List[Episode]:
        """Scrape podcast website for episodes"""
        # Implement specific scrapers for different podcast websites
        scrapers = {
            "markethuddle.substack.com": self._scrape_substack,
            "tim.blog": self._scrape_tim_ferriss,
            "hubermanlab.com": self._scrape_huberman,
            # Add more scrapers as needed
        }
        
        for domain, scraper in scrapers.items():
            if domain in website:
                return await scraper(podcast_name, website, days_back)
        
        return []
    
    async def _scrape_substack(self, podcast_name: str, website: str, days_back: int) -> List[Episode]:
        """Scrape Substack podcast pages"""
        try:
            # Get archive page
            archive_url = f"{website.rstrip('/')}/archive"
            response = self.session.get(archive_url, timeout=15)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            episodes = []
            cutoff = datetime.now() - timedelta(days=days_back)
            
            # Find podcast posts
            for post in soup.find_all('div', class_='post-preview'):
                # Check if it's a podcast
                if not post.find('div', class_='podcast-preview'):
                    continue
                
                title_elem = post.find('a', class_='post-preview-title')
                if not title_elem:
                    continue
                
                # Get post details
                post_url = title_elem['href']
                if not post_url.startswith('http'):
                    post_url = urljoin(website, post_url)
                
                # Get full post to find audio
                post_resp = self.session.get(post_url, timeout=10)
                post_soup = BeautifulSoup(post_resp.content, 'html.parser')
                
                # Find audio player
                audio_elem = post_soup.find('audio')
                if not audio_elem or not audio_elem.get('src'):
                    continue
                
                audio_url = audio_elem['src']
                
                # Parse date
                date_elem = post.find('time')
                if date_elem and date_elem.get('datetime'):
                    pub_date = datetime.fromisoformat(date_elem['datetime'].replace('Z', '+00:00'))
                    if pub_date.tzinfo:
                        pub_date = pub_date.replace(tzinfo=None)
                    
                    if pub_date < cutoff:
                        continue
                    
                    # Get full description from the post page
                    desc_elem = post_soup.find('div', class_='available-content')
                    if desc_elem:
                        description = desc_elem.get_text(separator=' ', strip=True)
                    else:
                        description = post.find('div', class_='post-preview-description').text.strip() if post.find('div', class_='post-preview-description') else ""
                    
                    episode = Episode(
                        podcast=podcast_name,
                        title=title_elem.text.strip(),
                        published=pub_date,
                        audio_url=audio_url,
                        link=post_url,
                        description=description
                    )
                    
                    episodes.append(episode)
            
            return episodes
            
        except Exception as e:
            logger.error(f"Substack scraping error: {e}")
            return []
    
    async def _try_direct_api(self, podcast_name: str, api_endpoint: str, days_back: int) -> List[Episode]:
        """Try direct API endpoint"""
        # Placeholder for direct API implementation
        return []
    
    async def _scrape_tim_ferriss(self, podcast_name: str, website: str, days_back: int) -> List[Episode]:
        """Scrape Tim Ferriss website"""
        # Placeholder for Tim Ferriss scraper
        return []
    
    async def _scrape_huberman(self, podcast_name: str, website: str, days_back: int) -> List[Episode]:
        """Scrape Huberman Lab website"""
        # Placeholder for Huberman scraper
        return []
    
    def _parse_date(self, entry) -> Optional[datetime]:
        """Parse date from feed entry"""
        # Try parsed date fields
        for field in ['published_parsed', 'updated_parsed', 'created_parsed']:
            if hasattr(entry, field) and getattr(entry, field):
                try:
                    return datetime(*getattr(entry, field)[:6])
                except:
                    continue
        
        # Try string dates
        for field in ['published', 'updated', 'pubDate']:
            if hasattr(entry, field) and getattr(entry, field):
                try:
                    from dateutil import parser
                    date = parser.parse(getattr(entry, field))
                    if date.tzinfo:
                        date = date.replace(tzinfo=None)
                    return date
                except:
                    continue
        
        return None
    
    def _extract_audio_url(self, entry) -> Optional[str]:
        """Extract audio URL from feed entry"""
        # Check enclosures
        if hasattr(entry, 'enclosures'):
            for enclosure in entry.enclosures:
                if enclosure.get('type', '').startswith('audio/'):
                    return enclosure.get('href') or enclosure.get('url')
                elif enclosure.get('href', '').lower().endswith(('.mp3', '.m4a', '.mp4')):
                    return enclosure.get('href')
        
        # Check links
        if hasattr(entry, 'links'):
            for link in entry.links:
                if link.get('type', '').startswith('audio/'):
                    return link.get('href')
                elif link.get('rel') == 'enclosure':
                    return link.get('href')
        
        return None
    
    def _extract_transcript_url(self, entry) -> Optional[str]:
        """Check if feed includes transcript URL"""
        # Some podcasts include transcript links in their feeds
        if hasattr(entry, 'links'):
            for link in entry.links:
                if 'transcript' in link.get('rel', '').lower():
                    return link.get('href')
                elif 'transcript' in link.get('title', '').lower():
                    return link.get('href')
        
        return None
    
    def _extract_full_description(self, entry) -> str:
        """Extract full description from feed entry without truncation"""
        description = ""
        
        # Try different fields that might contain the description
        for field in ['content', 'summary', 'description']:
            if hasattr(entry, field):
                value = getattr(entry, field)
                if isinstance(value, list) and value:
                    # Some feeds have content as a list of dicts
                    raw_desc = value[0].get('value', '')
                elif isinstance(value, str):
                    raw_desc = value
                else:
                    continue
                
                # Clean HTML from description
                soup = BeautifulSoup(raw_desc, 'html.parser')
                clean_desc = soup.get_text(separator=' ', strip=True)
                
                # Take the longest description we find
                if len(clean_desc) > len(description):
                    description = clean_desc
        
        # If no description found, return a default message
        if not description:
            description = "No description available for this episode."
        
        return description
    
    def _extract_description(self, entry) -> str:
        """Extract description from feed entry (keeping for backwards compatibility)"""
        return self._extract_full_description(entry)
    
    def _extract_duration(self, entry) -> str:
        """Extract duration from feed entry"""
        duration_str = None
        
        # Try different duration fields
        if hasattr(entry, 'itunes_duration'):
            duration_str = entry.itunes_duration
        elif hasattr(entry, 'duration'):
            duration_str = entry.duration
            
        if duration_str:
            return self._format_duration(duration_str)
        
        return "Unknown"
    
    def _format_duration(self, duration_str: str) -> str:
        """Format duration string into human-readable format"""
        if not duration_str or duration_str == "Unknown":
            return "Unknown"
        
        # Handle HH:MM:SS format
        if ':' in str(duration_str):
            parts = str(duration_str).split(':')
            try:
                if len(parts) == 3:  # HH:MM:SS
                    hours = int(parts[0])
                    minutes = int(parts[1])
                elif len(parts) == 2:  # MM:SS
                    hours = 0
                    minutes = int(parts[0])
                else:
                    return str(duration_str)
                
                if hours > 0:
                    return f"{hours} hour{'s' if hours > 1 else ''} {minutes} minute{'s' if minutes != 1 else ''}"
                else:
                    return f"{minutes} minute{'s' if minutes != 1 else ''}"
            except:
                return str(duration_str)
        
        # If already has text like "hour" or "minute", return as is
        if any(word in str(duration_str).lower() for word in ['hour', 'minute', 'second']):
            return str(duration_str)
        
        try:
            # Assume it's seconds if it's just a number
            if str(duration_str).isdigit():
                seconds = int(duration_str)
                hours = seconds // 3600
                minutes = (seconds % 3600) // 60
                
                if hours > 0:
                    return f"{hours} hour{'s' if hours > 1 else ''} {minutes} minute{'s' if minutes != 1 else ''}"
                else:
                    return f"{minutes} minute{'s' if minutes != 1 else ''}"
                    
        except:
            pass
        
        return str(duration_str)
    
    def _seconds_to_duration(self, seconds: int) -> str:
        """Convert seconds to duration string"""
        if seconds <= 0:
            return "Unknown"
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        if hours > 0:
            return f"{hours}h {minutes}m"
        else:
            return f"{minutes}m"


class RenaissanceWeekly:
    """Main application class"""
    
    def __init__(self):
        self.validate_env_vars()
        self.db = PodcastDatabase(DB_PATH)
        self.transcript_finder = TranscriptFinder(self.db)
        self.episode_fetcher = ReliableEpisodeFetcher(self.db)
        self.selected_episodes = []
    
    def validate_env_vars(self):
        """Validate required environment variables"""
        required = ["OPENAI_API_KEY", "SENDGRID_API_KEY"]
        missing = [var for var in required if not os.getenv(var)]
        
        if missing:
            raise ValueError(f"Missing required environment variables: {', '.join(missing)}")
        
        if not EMAIL_TO or EMAIL_TO == "caddington05@gmail.com":
            logger.info("📧 Using default email: caddington05@gmail.com")
            logger.info("💡 To change, set EMAIL_TO in your .env file")
    
    async def check_single_podcast(self, podcast_name: str, days_back: int = 7):
        """Debug function to check a single podcast"""
        # Find the podcast config
        podcast_config = None
        for config in PODCAST_CONFIGS:
            if config["name"].lower() == podcast_name.lower():
                podcast_config = config
                break
        
        if not podcast_config:
            logger.error(f"❌ Podcast '{podcast_name}' not found in configuration")
            logger.info("\nAvailable podcasts:")
            for config in PODCAST_CONFIGS:
                logger.info(f"  - {config['name']}")
            return
        
        logger.info(f"\n🔍 Checking {podcast_config['name']}")
        logger.info("="*60)
        
        # Try each RSS feed
        if "rss_feeds" in podcast_config:
            logger.info("\n📡 Testing RSS feeds:")
            for feed_url in podcast_config["rss_feeds"]:
                logger.info(f"\n  Feed: {feed_url}")
                try:
                    episodes = await self.episode_fetcher._try_rss_feeds(
                        podcast_config["name"], [feed_url], days_back
                    )
                    if episodes:
                        logger.info(f"  ✅ Success! Found {len(episodes)} episodes")
                        for ep in episodes[:2]:
                            logger.info(f"     - {ep.title} ({ep.published.strftime('%Y-%m-%d')})")
                    else:
                        logger.warning("  ❌ No episodes found")
                except Exception as e:
                    logger.error(f"  ❌ Error: {e}")
        
        # Check Apple Podcasts
        if "apple_id" in podcast_config:
            logger.info(f"\n🍎 Apple Podcasts ID: {podcast_config['apple_id']}")
            episodes = await self.episode_fetcher._try_apple_podcasts(
                podcast_config["name"], podcast_config["apple_id"], days_back
            )
            if episodes:
                logger.info(f"✅ Apple feed works! Found {len(episodes)} episodes")
            else:
                logger.warning("❌ Could not fetch from Apple")
        
        # Run full fetch
        logger.info("\n🚀 Running full episode fetch...")
        all_episodes = await self.episode_fetcher.fetch_episodes(podcast_config, days_back)
        
        if all_episodes:
            logger.info(f"\n✅ Total episodes found: {len(all_episodes)}")
            for ep in all_episodes:
                logger.info(f"  - {ep.title}")
                logger.info(f"    Published: {ep.published.strftime('%Y-%m-%d %H:%M')}")
                logger.info(f"    Duration: {ep.duration}")
                logger.info(f"    Has transcript: {'Yes' if ep.transcript_url else 'No'}")
                if len(ep.description) > 150:
                    logger.info(f"    Description: {ep.description[:150]}...")
                else:
                    logger.info(f"    Description: {ep.description}")
                logger.info("")
        else:
            logger.error("❌ No episodes found!")
        
        # Verify against Apple
        if "apple_id" in podcast_config:
            logger.info("\n📱 Verifying against Apple Podcasts...")
            verification = await self.episode_fetcher.verify_against_apple_podcasts(
                podcast_config, all_episodes, days_back
            )
            
            if verification["status"] == "success":
                logger.info(f"Apple episodes: {verification['apple_episode_count']}")
                logger.info(f"Found episodes: {verification['found_episode_count']}")
                logger.info(f"Missing episodes: {verification['missing_count']}")
                
                if verification["missing_count"] > 0:
                    logger.warning("\nMissing episodes:")
                    for ep in verification["missing_episodes"]:
                        logger.warning(f"  - {ep['title']} ({ep['date'].strftime('%Y-%m-%d')})")
            else:
                logger.error(f"Verification failed: {verification['reason']}")
    
    async def process_episode(self, episode: Episode) -> Optional[str]:
        """Process a single episode - find transcript or transcribe, then summarize"""
        logger.info(f"\n{'='*60}")
        logger.info(f"🎧 Processing: {episode.title}")
        logger.info(f"📅 Published: {episode.published.strftime('%Y-%m-%d')}")
        logger.info(f"🎙️  Podcast: {episode.podcast}")
        logger.info(f"{'='*60}")
        
        # Step 1: Try to find existing transcript
        transcript_text, transcript_source = await self.transcript_finder.find_transcript(episode)
        
        # Step 2: If no transcript found, transcribe from audio
        if not transcript_text:
            logger.info("📥 No transcript found - downloading audio for transcription...")
            transcript_file = await self.transcribe_from_audio(episode)
            
            if transcript_file and transcript_file.exists():
                with open(transcript_file, 'r', encoding='utf-8') as f:
                    transcript_text = f.read()
                transcript_source = TranscriptSource.GENERATED
            else:
                logger.error("❌ Failed to transcribe audio")
                return None
        
        # Save transcript to database
        self.db.save_episode(episode, transcript_text)
        
        # Step 3: Generate summary
        logger.info("📝 Generating executive summary...")
        summary = await self.generate_summary(episode, transcript_text, transcript_source)
        
        if summary:
            logger.info("✅ Episode processed successfully!")
        else:
            logger.error("❌ Failed to generate summary")
        
        return summary
    
    async def transcribe_from_audio(self, episode: Episode) -> Optional[Path]:
        """Download and transcribe audio"""
        if not episode.audio_url:
            logger.error("❌ No audio URL available")
            return None
        
        try:
            # Create unique filename with readable format
            date_str = episode.published.strftime('%Y%m%d')
            safe_podcast = self.slugify(episode.podcast)[:30]
            safe_title = self.slugify(episode.title)[:50]
            transcript_file = TRANSCRIPT_DIR / f"{date_str}_{safe_podcast}_{safe_title}_transcript.txt"
            
            # Check if already transcribed
            if transcript_file.exists():
                logger.info("✅ Found existing transcription")
                return transcript_file
            
            # Download audio
            logger.info("⬇️  Downloading audio...")
            audio_path = await self.download_audio(episode.audio_url)
            
            if not audio_path:
                return None
            
            # Transcribe with Whisper
            logger.info("🎯 Transcribing with Whisper...")
            transcript = await self.transcribe_with_whisper(audio_path)
            
            # Save transcript
            if transcript:
                with open(transcript_file, 'w', encoding='utf-8') as f:
                    f.write(transcript)
                logger.info(f"✅ Transcript saved: {transcript_file}")
                
                # Clean up audio file
                try:
                    os.remove(audio_path)
                except:
                    pass
                
                return transcript_file
            
        except Exception as e:
            logger.error(f"❌ Transcription error: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
        return None
    
    async def download_audio(self, audio_url: str) -> Optional[Path]:
        """Download audio file with better headers to avoid 403 errors"""
        try:
            from aiohttp import ClientSession
            
            # Create temp file
            temp_file = AUDIO_DIR / f"temp_{hashlib.md5(audio_url.encode()).hexdigest()[:8]}.mp3"
            
            # Better headers to avoid 403 errors
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'audio/mpeg, audio/mp4, audio/*;q=0.9, */*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'audio',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0'
            }
            
            async with ClientSession(headers=headers) as session:
                async with session.get(audio_url, allow_redirects=True) as response:
                    if response.status == 200:
                        total_size = int(response.headers.get('content-length', 0))
                        
                        async with aiofiles.open(temp_file, 'wb') as f:
                            downloaded = 0
                            async for chunk in response.content.iter_chunked(8192):
                                await f.write(chunk)
                                downloaded += len(chunk)
                                
                                if total_size > 0:
                                    progress = (downloaded / total_size) * 100
                                    print(f"\r  Progress: {progress:.1f}%", end='', flush=True)
                        
                        print()  # New line after progress
                        logger.info(f"✅ Downloaded: {downloaded/1_000_000:.1f}MB")
                        return temp_file
                    else:
                        logger.error(f"❌ Download failed: HTTP {response.status}")
                        # Try alternate download method if 403
                        if response.status == 403:
                            return await self._alternate_download(audio_url)
                        
        except Exception as e:
            logger.error(f"❌ Download error: {e}")
        
        return None
    
    async def _alternate_download(self, audio_url: str) -> Optional[Path]:
        """Alternate download method using requests library"""
        try:
            logger.info("🔄 Trying alternate download method...")
            
            # Create temp file
            temp_file = AUDIO_DIR / f"temp_{hashlib.md5(audio_url.encode()).hexdigest()[:8]}.mp3"
            
            # Use requests with session
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Renaissance Weekly Podcast Bot/2.0 (Compatible; Like iTunes)',
                'Accept': '*/*',
                'Accept-Encoding': 'identity',
                'Connection': 'keep-alive',
            })
            
            response = session.get(audio_url, stream=True, allow_redirects=True)
            
            if response.status_code == 200:
                total_size = int(response.headers.get('content-length', 0))
                downloaded = 0
                
                with open(temp_file, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)
                            
                            if total_size > 0:
                                progress = (downloaded / total_size) * 100
                                print(f"\r  Progress: {progress:.1f}%", end='', flush=True)
                
                print()  # New line after progress
                logger.info(f"✅ Downloaded via alternate method: {downloaded/1_000_000:.1f}MB")
                return temp_file
            else:
                logger.error(f"❌ Alternate download failed: HTTP {response.status_code}")
                
        except Exception as e:
            logger.error(f"❌ Alternate download error: {e}")
        
        return None
    
    async def transcribe_with_whisper(self, audio_path: Path) -> Optional[str]:
        """Transcribe audio using OpenAI Whisper with better error handling and retry logic"""
        try:
            # Load and process audio
            audio = AudioSegment.from_file(audio_path)
            duration_min = len(audio) / 60000
            
            logger.info(f"⏱️  Duration: {duration_min:.1f} minutes")
            
            # Apply testing limit if enabled
            if TESTING_MODE and duration_min > MAX_TRANSCRIPTION_MINUTES:
                logger.info(f"🧪 TESTING MODE: Limiting to {MAX_TRANSCRIPTION_MINUTES} minutes")
                audio = audio[:MAX_TRANSCRIPTION_MINUTES * 60 * 1000]
            
            # Create chunks for Whisper (25MB limit, but we'll use 20MB to be safe)
            chunks = self._create_audio_chunks(audio)
            logger.info(f"📦 Created {len(chunks)} chunks for transcription")
            
            transcripts = []
            
            for i, chunk_path in enumerate(chunks):
                logger.info(f"🎙️  Transcribing chunk {i+1}/{len(chunks)}...")
                
                # Retry logic for each chunk
                max_retries = 3
                retry_delay = 5  # seconds
                
                for attempt in range(max_retries):
                    try:
                        with open(chunk_path, 'rb') as audio_file:
                            # Use the client with timeout already configured
                            transcript = openai_client.audio.transcriptions.create(
                                model="whisper-1",
                                file=audio_file,
                                response_format="text"
                            )
                        
                        transcripts.append(transcript.strip())
                        logger.info(f"   ✅ Chunk {i+1} transcribed successfully")
                        break  # Success, exit retry loop
                        
                    except openai.APITimeoutError as e:
                        logger.warning(f"   ⏱️ Timeout on chunk {i+1}, attempt {attempt+1}/{max_retries}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(retry_delay)
                            retry_delay *= 2  # Exponential backoff
                        else:
                            logger.error(f"   ❌ Failed to transcribe chunk {i+1} after {max_retries} attempts")
                            raise
                            
                    except openai.RateLimitError as e:
                        logger.warning(f"   ⚡ Rate limit hit on chunk {i+1}")
                        wait_time = 60  # Default wait time
                        # Try to parse wait time from error message
                        if 'Please try again in' in str(e):
                            try:
                                wait_match = re.search(r'in (\d+)s', str(e))
                                if wait_match:
                                    wait_time = int(wait_match.group(1)) + 1
                            except:
                                pass
                        logger.info(f"   ⏳ Waiting {wait_time} seconds before retry...")
                        await asyncio.sleep(wait_time)
                        
                    except openai.APIError as e:
                        logger.error(f"   ❌ API error on chunk {i+1}: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(retry_delay)
                        else:
                            raise
                            
                    except Exception as e:
                        logger.error(f"   ❌ Unexpected error on chunk {i+1}: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(retry_delay)
                        else:
                            raise
                
                # Clean up chunk after successful transcription
                try:
                    os.remove(chunk_path)
                except:
                    pass
                
                # Small delay between chunks to avoid rate limits
                if i < len(chunks) - 1:
                    await asyncio.sleep(2)
            
            # Merge transcripts with better handling
            if not transcripts:
                logger.error("❌ No transcripts generated")
                return None
                
            full_transcript = " ".join(transcripts)
            
            # Basic quality check
            if len(full_transcript) < 100:
                logger.warning(f"⚠️  Transcript seems too short: {len(full_transcript)} characters")
                
            logger.info(f"✅ Transcription complete: {len(full_transcript)} characters")
            
            return full_transcript
            
        except Exception as e:
            logger.error(f"❌ Whisper transcription error: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # Clean up any remaining chunks
            try:
                for file in TEMP_DIR.glob("chunk_*.mp3"):
                    file.unlink()
            except:
                pass
                
            return None
    
    def _create_audio_chunks(self, audio: AudioSegment, max_size_mb: int = 20) -> List[Path]:
        """Split audio into chunks for Whisper API"""
        chunks = []
        chunk_duration_ms = 20 * 60 * 1000  # 20 minutes
        
        for i in range(0, len(audio), chunk_duration_ms):
            chunk = audio[i:i + chunk_duration_ms]
            
            # Export chunk
            chunk_path = TEMP_DIR / f"chunk_{i//chunk_duration_ms}.mp3"
            chunk.export(chunk_path, format="mp3", bitrate="64k")
            
            # Check size
            if os.path.getsize(chunk_path) > max_size_mb * 1024 * 1024:
                # Re-export with lower quality
                os.remove(chunk_path)
                chunk.export(chunk_path, format="mp3", bitrate="32k")
            
            chunks.append(chunk_path)
        
        return chunks
    
    async def generate_summary(self, episode: Episode, transcript: str, source: TranscriptSource) -> Optional[str]:
        """Generate executive summary using GPT-4o"""
        try:
            # Create safe filename for summary with readable format
            date_str = episode.published.strftime('%Y%m%d')
            safe_podcast = self.slugify(episode.podcast)[:30]
            safe_title = self.slugify(episode.title)[:50]
            summary_file = SUMMARY_DIR / f"{date_str}_{safe_podcast}_{safe_title}_summary.md"
            
            if summary_file.exists():
                logger.info("✅ Found cached summary")
                with open(summary_file, 'r', encoding='utf-8') as f:
                    return f.read()
            
            # Executive summary prompt
            prompt = f"""EPISODE: {episode.title}
PODCAST: {episode.podcast}
TRANSCRIPT SOURCE: {source.value}

TRANSCRIPT:
{transcript[:100000]}  # Limit for API

You are creating an executive briefing for Renaissance Weekly readers - busy professionals, founders, investors, and thought leaders who value their time above all else. This is not a generic summary; it's a strategic distillation that respects both the depth of the conversation and the reader's need for actionable intelligence.

Your task: Transform this podcast into a compelling narrative that captures not just what was said, but why it matters for someone building the future.

STRUCTURE YOUR SUMMARY AS FOLLOWS:

## Executive Summary & Guest Profile
Start with a 3-4 sentence executive summary that captures the absolute essence of this conversation. What is the ONE transformative insight that changes how we think about the topic?

Then provide a rich 2-3 paragraph profile of the guest that goes beyond credentials. Who is this person really? What unique vantage point do they bring? What makes their perspective invaluable? Include their most impressive achievements, their contrarian positions, and why Renaissance Weekly readers should care about their worldview.

## The Core Argument
In 2-3 flowing paragraphs, articulate the central thesis of this conversation. This is not a list of topics discussed, but a narrative explanation of the big idea that emerges. What worldview is being advanced? What conventional wisdom is being challenged? Write this as you would explain it to a brilliant friend over coffee.

## Key Insights That Matter
Present 5-7 insights, but make each one substantial (3-4 sentences). These should be:
- Counterintuitive or perspective-shifting
- Backed by specific examples or data from the conversation
- Written with vivid language that makes abstract concepts concrete
- Focused on insights that change how we act, not just how we think

Format each with a bold header that captures the essence, like:
**The 10,000 hour rule is dead - here's what actually drives mastery**
Then explain the insight with specificity and nuance.

## The Moment of Revelation
Identify and describe 1-2 pivotal moments in the conversation where a genuinely surprising insight emerged. Set the scene - what question prompted it? How did the guest's demeanor change? Quote the key exchange and explain why this moment matters. This brings the reader into the room.

## Practical Frameworks & Mental Models
Extract 3-4 concrete frameworks or mental models discussed. For each:
- Name it memorably
- Explain how it works in 2-3 sentences
- Provide a specific example of application
- Connect it to broader principles

## What You Can Do This Week
5-6 specific actions, but make them sophisticated and contextualized:
- Not just "try meditation" but "implement the 4-7-8 breathing protocol before high-stakes meetings"
- Include the why and expected outcome
- Range from 5-minute experiments to longer-term implementations
- Focus on high-leverage activities that compound

## The Deeper Game Being Played
A short, reflective paragraph that zooms out to the meta-level. What's really at stake in this conversation? How does it connect to larger trends in technology, society, or human performance? This is where you earn the reader's trust as a curator of ideas.

## Resources & Rabbit Holes
Organize resources intelligently:
**Essential Reading**: 2-3 books with one-sentence explanations of their core value
**Tools for Implementation**: Specific apps/services with use cases
**Further Exploration**: Related thinkers, concepts, or episodes to explore
**The Guest's Work**: Where to follow up directly

Remember: Your readers chose Renaissance Weekly because they refuse to choose between breadth and depth. Give them both. Write with the precision of The Economist, the insight of The New Yorker, and the practical value of the best business writing. Every sentence should either inform, inspire, or instruct.

Avoid corporate jargon, buzzwords, and filler. Use vivid, precise language. If you wouldn't say it to a brilliant friend, don't write it here."""
            
            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are the lead writer for Renaissance Weekly, a premium newsletter that serves the intellectually ambitious. Your readers include founders, investors, scientists, and polymaths who expect world-class curation and insight. You have a gift for finding the profound in the practical and making complex ideas irresistibly clear."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=4000,
                temperature=0.3
            )
            
            summary = response.choices[0].message.content
            
            # Add metadata
            summary += f"\n\n---\n\n"
            summary += f"**Episode**: {episode.title}\n"
            summary += f"**Podcast**: {episode.podcast}\n"
            summary += f"**Published**: {episode.published.strftime('%Y-%m-%d')}\n"
            summary += f"**Transcript Source**: {source.value}\n"
            if episode.link:
                summary += f"**Link**: [{episode.title}]({episode.link})\n"
            
            # Cache summary
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(summary)
            
            return summary
            
        except Exception as e:
            logger.error(f"❌ Summary generation error: {e}")
            return None
    
    def slugify(self, text: str) -> str:
        """Convert text to filename-safe string"""
        # Remove or replace invalid filename characters
        invalid_chars = '<>:"/\\|?*'
        safe_text = text
        for char in invalid_chars:
            safe_text = safe_text.replace(char, '_')
        
        # Replace multiple underscores with single
        safe_text = re.sub(r'_+', '_', safe_text)
        
        # Limit length and clean up
        safe_text = safe_text[:100].strip('_')
        
        return safe_text
    
    async def run_verification_report(self, days_back: int = 7):
        """Run a verification report comparing all sources against Apple Podcasts"""
        logger.info("🔍 Running Apple Podcasts Verification Report...")
        logger.info(f"📅 Checking episodes from the last {days_back} days\n")
        
        report_data = []
        total_found = 0
        total_missing = 0
        
        for podcast_config in PODCAST_CONFIGS:
            podcast_name = podcast_config["name"]
            logger.info(f"Checking {podcast_name}...")
            
            # Fetch episodes using our methods
            episodes = await self.episode_fetcher.fetch_episodes(podcast_config, days_back)
            
            # Verify against Apple
            verification = await self.episode_fetcher.verify_against_apple_podcasts(
                podcast_config, episodes, days_back
            )
            
            report_entry = {
                "podcast": podcast_name,
                "found_episodes": len(episodes),
                "verification": verification
            }
            
            if verification["status"] == "success":
                report_entry["apple_episodes"] = verification["apple_episode_count"]
                report_entry["missing_episodes"] = verification["missing_count"]
                total_found += len(episodes)
                total_missing += verification["missing_count"]
            
            report_data.append(report_entry)
        
        # Generate report
        logger.info("\n" + "="*80)
        logger.info("📊 VERIFICATION REPORT SUMMARY")
        logger.info("="*80)
        
        for entry in report_data:
            podcast = entry["podcast"]
            found = entry["found_episodes"]
            verification = entry["verification"]
            
            if verification["status"] == "success":
                apple_count = verification["apple_episode_count"]
                missing = verification["missing_count"]
                
                status = "✅" if missing == 0 else "⚠️"
                logger.info(f"\n{status} {podcast}")
                logger.info(f"   Found: {found} episodes")
                logger.info(f"   Apple: {apple_count} episodes")
                
                if missing > 0:
                    logger.warning(f"   Missing: {missing} episodes")
                    for i, ep in enumerate(verification["missing_episodes"][:3]):
                        logger.warning(f"      - {ep['title']} ({ep['date'].strftime('%Y-%m-%d')})")
                    if missing > 3:
                        logger.warning(f"      ... and {missing - 3} more")
                    
                    # Show the Apple feed URL for manual checking
                    logger.info(f"   Apple Feed: {verification['apple_feed_url']}")
            else:
                logger.warning(f"\n❌ {podcast}")
                logger.warning(f"   Verification failed: {verification['reason']}")
                logger.info(f"   Found: {found} episodes (unverified)")
        
        logger.info("\n" + "="*80)
        logger.info(f"📈 TOTALS:")
        logger.info(f"   Episodes found: {total_found}")
        logger.info(f"   Episodes missing: {total_missing}")
        logger.info(f"   Success rate: {(total_found/(total_found+total_missing)*100):.1f}%" if total_found + total_missing > 0 else "N/A")
        logger.info("="*80)
        
        # Save detailed report
        report_file = Path("verification_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, default=str)
        logger.info(f"\n💾 Detailed report saved to: {report_file}")
    
    async def run(self, days_back: int = 7):
        """Main execution function"""
        logger.info("🚀 Starting Renaissance Weekly System...")
        logger.info(f"📧 Email delivery: {EMAIL_FROM} → {EMAIL_TO}")
        logger.info(f"📅 Looking back {days_back} days")
        
        if TESTING_MODE:
            logger.info(f"🧪 TESTING MODE: Limited to {MAX_TRANSCRIPTION_MINUTES} min transcriptions")
        
        # Fetch episodes from all configured podcasts
        all_episodes = []
        verification_results = {}
        podcast_episodes_map = {}  # Track episodes per podcast for verification
        
        for podcast_config in PODCAST_CONFIGS:
            episodes = await self.episode_fetcher.fetch_episodes(podcast_config, days_back)
            podcast_episodes_map[podcast_config["name"]] = episodes
            all_episodes.extend(episodes)
            
            # Verify against Apple Podcasts if verification is enabled
            if os.getenv("VERIFY_APPLE_PODCASTS", "true").lower() == "true":
                verification = await self.episode_fetcher.verify_against_apple_podcasts(
                    podcast_config, episodes, days_back
                )
                verification_results[podcast_config["name"]] = verification
        
        # Try to fetch missing episodes if enabled
        if os.getenv("FETCH_MISSING_EPISODES", "true").lower() == "true" and verification_results:
            logger.info("\n🔄 Attempting to fetch missing episodes...")
            
            for podcast_config in PODCAST_CONFIGS:
                podcast_name = podcast_config["name"]
                if podcast_name in verification_results:
                    result = verification_results[podcast_name]
                    if result["status"] == "success" and result["missing_count"] > 0:
                        existing = podcast_episodes_map[podcast_name]
                        additional = await self.episode_fetcher.fetch_missing_from_apple(
                            podcast_config, existing, result
                        )
                        if additional:
                            all_episodes.extend(additional)
                            logger.info(f"   Added {len(additional)} episodes for {podcast_name}")
        
        if not all_episodes:
            logger.error("❌ No recent episodes found")
            return
        
        logger.info(f"✅ Found {len(all_episodes)} total episodes")
        
        # Display verification summary if enabled
        if verification_results:
            logger.info("\n📱 Apple Podcasts Verification Summary:")
            total_missing = 0
            for podcast_name, result in verification_results.items():
                if result["status"] == "success" and result["missing_count"] > 0:
                    logger.warning(f"   {podcast_name}: Missing {result['missing_count']} episodes")
                    total_missing += result["missing_count"]
                elif result["status"] == "error":
                    logger.debug(f"   {podcast_name}: Verification failed - {result['reason']}")
            
            if total_missing > 0:
                logger.warning(f"\n⚠️  Total missing episodes across all podcasts: {total_missing}")
                if os.getenv("FETCH_MISSING_EPISODES", "true").lower() != "true":
                    logger.info("💡 Set FETCH_MISSING_EPISODES=true to automatically fetch missing episodes")
        
        # Episode selection UI
        selected_episodes = self.run_selection_server(all_episodes)
        
        if not selected_episodes:
            logger.warning("❌ No episodes selected")
            return
        
        logger.info(f"🎯 Processing {len(selected_episodes)} selected episodes...")
        
        # Process episodes concurrently
        summaries = []
        summary_episodes = []  # Track which episodes have successful summaries
        
        async def process_with_semaphore(episode, semaphore):
            async with semaphore:
                summary = await self.process_episode(episode)
                return (episode, summary)
        
        semaphore = asyncio.Semaphore(3)  # Max 3 concurrent
        
        tasks = [
            process_with_semaphore(Episode(**ep) if isinstance(ep, dict) else ep, semaphore)
            for ep in selected_episodes
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Episode {i+1} failed: {result}")
            elif isinstance(result, tuple) and result[1]:  # (episode, summary)
                episode, summary = result
                summaries.append(summary)
                summary_episodes.append(episode)
        
        logger.info(f"✅ Successfully processed {len(summaries)}/{len(selected_episodes)} episodes")
        
        # Send email digest
        if summaries:
            if self.send_summary_email(summaries, summary_episodes):
                logger.info("📧 Renaissance Weekly digest sent!")
            else:
                logger.error("❌ Failed to send email")
        else:
            logger.warning("❌ No summaries generated - nothing to send")
    
    def run_selection_server(self, episodes: List[Episode]) -> List[Episode]:
        """Run a temporary web server for episode selection"""
        selected_episodes = []
        server_running = True
        parent_instance = self
        
        class SelectionHandler(SimpleHTTPRequestHandler):
            def do_GET(self):
                if self.path == '/':
                    self.send_response(200)
                    self.send_header('Content-type', 'text/html')
                    self.end_headers()
                    html = parent_instance.create_selection_html(episodes)
                    self.wfile.write(html.encode())
                else:
                    self.send_error(404)
            
            def do_POST(self):
                if self.path == '/select':
                    content_length = int(self.headers['Content-Length'])
                    post_data = self.rfile.read(content_length)
                    data = json.loads(post_data.decode('utf-8'))
                    
                    for idx in data['selected']:
                        selected_episodes.append(episodes[idx])
                    
                    self.send_response(200)
                    self.end_headers()
                    
                    nonlocal server_running
                    server_running = False
            
            def log_message(self, format, *args):
                pass  # Suppress logs
        
        # Find available port
        port = 8888
        server = None
        
        for attempt in range(5):
            try:
                server = HTTPServer(('localhost', port), SelectionHandler)
                break
            except OSError:
                port += 1
                if attempt == 4:
                    logger.error("Could not start web server")
                    return self._fallback_text_selection(episodes)
        
        if not server:
            return self._fallback_text_selection(episodes)
        
        # Open browser
        url = f'http://localhost:{port}'
        logger.info(f"🌐 Opening episode selection at {url}")
        
        try:
            webbrowser.open(url)
        except:
            logger.warning(f"Please open: {url}")
        
        # Run server
        logger.info("⏳ Waiting for episode selection...")
        
        try:
            while server_running:
                server.handle_request()
        except KeyboardInterrupt:
            logger.warning("Selection cancelled")
            selected_episodes = []
        finally:
            server.server_close()
        
        return selected_episodes
    
    def create_selection_html(self, episodes: List[Episode]) -> str:
        """Create an HTML page for episode selection with full descriptions"""
        from html import escape
        
        # Create podcast description lookup
        podcast_descriptions = {}
        for config in PODCAST_CONFIGS:
            podcast_descriptions[config["name"]] = config.get("description", "")
        
        html = """<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Renaissance Weekly - Episode Selection</title>
    <style>
        * { box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }
        h1 { 
            color: #333; 
            margin-bottom: 10px;
            font-size: 36px;
        }
        .subtitle { 
            color: #666; 
            margin-bottom: 30px; 
            font-size: 18px; 
        }
        .podcast-group {
            background: white;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .podcast-header {
            display: flex;
            align-items: baseline;
            gap: 15px;
            margin-bottom: 15px;
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 10px;
        }
        .podcast-title {
            font-size: 20px;
            font-weight: 600;
            color: #2c3e50;
        }
        .podcast-description {
            font-size: 14px;
            color: #666;
            font-style: italic;
        }
        .episode {
            padding: 15px;
            margin-bottom: 10px;
            border: 1px solid #e0e0e0;
            border-radius: 6px;
            transition: all 0.2s ease;
            cursor: pointer;
        }
        .episode:hover { 
            background-color: #f8f9fa; 
            border-color: #4a90e2; 
        }
        .episode.selected { 
            background-color: #e3f2fd; 
            border-color: #2196f3; 
        }
        .episode-header { 
            display: flex; 
            align-items: flex-start; 
            gap: 15px; 
        }
        input[type="checkbox"] { 
            width: 20px; 
            height: 20px; 
            margin-top: 2px; 
            cursor: pointer;
            flex-shrink: 0;
        }
        .episode-content { 
            flex: 1;
            min-width: 0; /* Allow text to wrap */
        }
        .episode-title { 
            font-weight: 500; 
            color: #333; 
            margin-bottom: 5px; 
            font-size: 16px;
            word-wrap: break-word;
        }
        .episode-meta { 
            font-size: 14px; 
            color: #666; 
            margin-bottom: 8px; 
        }
        .episode-description { 
            font-size: 14px; 
            color: #555; 
            line-height: 1.6;
            word-wrap: break-word;
        }
        .transcript-badge {
            display: inline-block;
            padding: 2px 8px;
            background: #4CAF50;
            color: white;
            font-size: 12px;
            border-radius: 4px;
            margin-left: 10px;
            font-weight: normal;
        }
        .controls {
            position: sticky;
            top: 20px;
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            margin-bottom: 30px;
            z-index: 100;
        }
        .button {
            padding: 12px 24px;
            margin: 5px;
            border: none;
            border-radius: 6px;
            font-size: 16px;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s ease;
        }
        .button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        .button-primary { 
            background-color: #4a90e2; 
            color: white; 
        }
        .button-primary:hover:not(:disabled) { 
            background-color: #357abd; 
        }
        .button-secondary { 
            background-color: #6c757d; 
            color: white; 
        }
        .button-secondary:hover:not(:disabled) { 
            background-color: #545b62; 
        }
        .selection-count { 
            font-size: 16px; 
            color: #666; 
            margin-left: 20px; 
        }
        .loading {
            display: none;
            text-align: center;
            padding: 40px;
            font-size: 18px;
            color: #666;
        }
        .error-message {
            color: #d32f2f;
            padding: 10px;
            margin: 10px 0;
            background: #ffebee;
            border-radius: 4px;
            display: none;
        }
        @media (max-width: 768px) {
            body { padding: 10px; }
            h1 { font-size: 28px; }
            .controls { position: static; }
            .button { 
                display: block; 
                width: 100%; 
                margin: 5px 0; 
            }
            .podcast-header {
                flex-direction: column;
                gap: 5px;
            }
        }
    </style>
</head>
<body>
    <h1>🎙️ Renaissance Weekly</h1>
    <p class="subtitle">Select episodes to process for this week's digest</p>
    
    <div class="controls">
        <button class="button button-primary" onclick="processSelected()">Process Selected Episodes</button>
        <button class="button button-secondary" onclick="selectAll()">Select All</button>
        <button class="button button-secondary" onclick="selectNone()">Clear All</button>
        <span class="selection-count">0 episodes selected</span>
    </div>
    
    <div class="error-message" id="error-message"></div>
    
    <div class="loading" id="loading">
        Processing your selection... This window will close automatically.
    </div>
    
    <div id="episodes">
"""
        
        # Group episodes by podcast
        by_podcast = {}
        for i, ep in enumerate(episodes):
            try:
                if isinstance(ep, Episode):
                    podcast = ep.podcast
                    title = ep.title
                    published = ep.published.strftime('%Y-%m-%d')
                    duration = ep.duration
                    has_transcript = ep.transcript_url is not None
                    description = ep.description
                else:
                    podcast = ep.get('podcast', 'Unknown')
                    title = ep.get('title', 'Unknown')
                    published = ep.get('published', 'Unknown')
                    if isinstance(published, datetime):
                        published = published.strftime('%Y-%m-%d')
                    duration = ep.get('duration', 'Unknown')
                    has_transcript = ep.get('transcript_url') is not None
                    description = ep.get('description', '')
                
                if podcast not in by_podcast:
                    by_podcast[podcast] = []
                
                # Process description to get first two sentences
                if description:
                    # Clean HTML/Markdown first
                    clean_desc = re.sub(r'<[^>]+>', '', description)  # Remove HTML tags
                    clean_desc = re.sub(r'\*{1,2}([^\*]+)\*{1,2}', r'\1', clean_desc)  # Remove markdown bold/italic
                    clean_desc = clean_desc.strip()
                    
                    # Split into sentences and take first two
                    sentences = re.split(r'(?<=[.!?])\s+', clean_desc)
                    if len(sentences) >= 2:
                        display_desc = ' '.join(sentences[:2])
                        # Ensure it ends with punctuation
                        if display_desc and display_desc[-1] not in '.!?':
                            display_desc += '.'
                    else:
                        display_desc = clean_desc
                        if display_desc and display_desc[-1] not in '.!?':
                            display_desc += '.'
                else:
                    display_desc = "No description available for this episode. Click to select for processing."
                
                # Format duration for display
                if isinstance(duration, str) and duration.isdigit():
                    seconds = int(duration)
                    hours = seconds // 3600
                    minutes = (seconds % 3600) // 60
                    if hours > 0:
                        duration_display = f"{hours} hour{'s' if hours > 1 else ''}, {minutes} minute{'s' if minutes != 1 else ''}"
                    else:
                        duration_display = f"{minutes} minute{'s' if minutes != 1 else ''}"
                else:
                    duration_display = duration
                
                by_podcast[podcast].append({
                    'index': i,
                    'title': title,
                    'published': published,
                    'duration': duration_display,
                    'has_transcript': has_transcript,
                    'description': display_desc
                })
            except Exception as e:
                logger.error(f"Error processing episode {i}: {e}")
                continue
        
        # Create HTML for each podcast group
        for podcast, podcast_episodes in by_podcast.items():
            podcast_desc = podcast_descriptions.get(podcast, "")
            
            html += f'<div class="podcast-group">\n'
            html += f'<div class="podcast-header">\n'
            html += f'<div class="podcast-title">{escape(podcast)}</div>\n'
            if podcast_desc:
                html += f'<div class="podcast-description">— {escape(podcast_desc)}</div>\n'
            html += f'</div>\n'
            
            for ep_data in podcast_episodes:
                transcript_badge = '<span class="transcript-badge">TRANSCRIPT</span>' if ep_data['has_transcript'] else ''
                
                html += f'''<div class="episode" id="episode-{ep_data['index']}" onclick="toggleEpisode({ep_data['index']})">
    <div class="episode-header">
        <input type="checkbox" id="cb-{ep_data['index']}" value="{ep_data['index']}" onclick="event.stopPropagation();" onchange="updateSelection()">
        <div class="episode-content">
            <div class="episode-title">{escape(ep_data['title'])}{transcript_badge}</div>
            <div class="episode-meta">📅 {escape(str(ep_data['published']))} | ⏱️ {escape(str(ep_data['duration']))}</div>
            <div class="episode-description">{escape(ep_data['description'])}</div>
        </div>
    </div>
</div>\n'''
            
            html += '</div>\n'
        
        html += """
    </div>
    
    <script>
        function toggleEpisode(index) {
            const checkbox = document.getElementById('cb-' + index);
            checkbox.checked = !checkbox.checked;
            updateSelection();
        }
        
        function updateSelection() {
            const checkboxes = document.querySelectorAll('input[type="checkbox"]');
            let count = 0;
            checkboxes.forEach(cb => {
                const episode = document.getElementById('episode-' + cb.value);
                if (cb.checked) {
                    count++;
                    episode.classList.add('selected');
                } else {
                    episode.classList.remove('selected');
                }
            });
            document.querySelector('.selection-count').textContent = count + ' episode' + (count !== 1 ? 's' : '') + ' selected';
        }
        
        function selectAll() {
            document.querySelectorAll('input[type="checkbox"]').forEach(cb => cb.checked = true);
            updateSelection();
        }
        
        function selectNone() {
            document.querySelectorAll('input[type="checkbox"]').forEach(cb => cb.checked = false);
            updateSelection();
        }
        
        function showError(message) {
            const errorDiv = document.getElementById('error-message');
            errorDiv.textContent = message;
            errorDiv.style.display = 'block';
            setTimeout(() => {
                errorDiv.style.display = 'none';
            }, 5000);
        }
        
        function processSelected() {
            const selected = [];
            document.querySelectorAll('input[type="checkbox"]:checked').forEach(cb => {
                selected.push(parseInt(cb.value));
            });
            
            if (selected.length === 0) {
                showError('Please select at least one episode to process.');
                return;
            }
            
            // Disable all buttons
            document.querySelectorAll('.button').forEach(btn => btn.disabled = true);
            
            document.getElementById('loading').style.display = 'block';
            document.getElementById('episodes').style.display = 'none';
            document.querySelector('.controls').style.display = 'none';
            
            fetch('/select', {
                method: 'POST',
                headers: {'Content-Type': 'application/json'},
                body: JSON.stringify({selected: selected})
            })
            .then(response => {
                if (!response.ok) {
                    throw new Error('Server error');
                }
                setTimeout(() => window.close(), 1000);
            })
            .catch(error => {
                document.getElementById('loading').style.display = 'none';
                document.getElementById('episodes').style.display = 'block';
                document.querySelector('.controls').style.display = 'block';
                document.querySelectorAll('.button').forEach(btn => btn.disabled = false);
                showError('Failed to process selection. Please try again.');
            });
        }
        
        // Initialize on load
        document.addEventListener('DOMContentLoaded', function() {
            updateSelection();
        });
    </script>
</body>
</html>
"""
        
        return html
    
    def _format_duration_display(self, duration_str: str) -> str:
        """Format duration string for display"""
        if not duration_str or duration_str == "Unknown":
            return "Unknown"
        
        # Handle HH:MM:SS format
        if ':' in str(duration_str):
            parts = str(duration_str).split(':')
            try:
                if len(parts) == 3:  # HH:MM:SS
                    hours = int(parts[0])
                    minutes = int(parts[1])
                elif len(parts) == 2:  # MM:SS
                    hours = 0
                    minutes = int(parts[0])
                else:
                    return str(duration_str)
                
                if hours > 0:
                    return f"{hours} hour{'s' if hours != 1 else ''} {minutes} minute{'s' if minutes != 1 else ''}"
                else:
                    return f"{minutes} minute{'s' if minutes != 1 else ''}"
            except:
                return str(duration_str)
        
        # If already has text like "hour" or "minute", return as is
        if any(word in str(duration_str).lower() for word in ['hour', 'minute', 'second']):
            return str(duration_str)
        
        return str(duration_str)
    
    def _fallback_text_selection(self, episodes: List[Episode]) -> List[Episode]:
        """Text-based fallback selection method"""
        print("\n" + "="*80)
        print("📻 RECENT PODCAST EPISODES (Text Selection)")
        print("="*80)
        
        episode_map = {}
        for i, ep in enumerate(episodes):
            if isinstance(ep, Episode):
                print(f"\n[{i+1}] {ep.podcast}: {ep.title}")
                print(f"    📅 {ep.published.strftime('%Y-%m-%d')} | ⏱️  {ep.duration}")
                if ep.transcript_url:
                    print(f"    ✅ Transcript available")
            else:
                print(f"\n[{i+1}] {ep['podcast']}: {ep['title']}")
                print(f"    📅 {ep['published']} | ⏱️  {ep['duration']}")
            episode_map[i+1] = ep
        
        print("\n" + "="*80)
        print("Enter episode numbers separated by commas (e.g., 1,3,5)")
        print("Or type 'all' for all episodes, 'none' to exit")
        
        while True:
            selection = input("\n🎯 Your selection: ").strip().lower()
            
            if selection == 'none':
                return []
            
            if selection == 'all':
                return episodes
            
            try:
                if not selection:
                    print("❌ Please enter episode numbers or 'all'/'none'")
                    continue
                
                selected_indices = [int(x.strip()) for x in selection.split(',') if x.strip()]
                
                invalid = [i for i in selected_indices if i not in episode_map]
                if invalid:
                    print(f"❌ Invalid episode numbers: {invalid}")
                    continue
                
                selected_episodes = [episode_map[i] for i in selected_indices]
                
                print(f"\n✅ Selected {len(selected_episodes)} episode(s)")
                return selected_episodes
                
            except ValueError:
                print("❌ Invalid input. Please enter numbers separated by commas.")
    
    def send_summary_email(self, summaries: List[str], episodes: List[Episode]) -> bool:
        """Send Renaissance Weekly digest"""
        try:
            logger.info("📧 Preparing Renaissance Weekly digest...")
            
            # Create email content
            html_content = self.create_substack_style_email(summaries, episodes)
            plain_content = self._create_plain_text_version(summaries)
            
            # Create subject with correct count
            subject = f"Renaissance Weekly: {len(summaries)} Essential Conversation{'s' if len(summaries) != 1 else ''}"
            
            # Create message
            message = Mail(
                from_email=(EMAIL_FROM, "Renaissance Weekly"),
                to_emails=EMAIL_TO,
                subject=subject,
                plain_text_content=plain_content,
                html_content=html_content
            )
            
            # Send email
            response = sendgrid_client.send(message)
            
            if response.status_code == 202:
                logger.info("✅ Email sent successfully!")
                return True
            else:
                logger.error(f"Email failed: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"Email error: {e}")
            return False
    
    def _convert_markdown_to_html(self, markdown: str) -> str:
        """Convert markdown to HTML with proper handling"""
        import re
        from html import escape
        
        # First, escape any existing HTML to prevent injection
        lines = markdown.split('\n')
        html_lines = []
        in_code_block = False
        current_paragraph = []
        
        for line in lines:
            # Handle code blocks
            if line.strip().startswith('```'):
                if current_paragraph:
                    html_lines.append('<p style="margin: 0 0 20px 0;">' + ' '.join(current_paragraph) + '</p>')
                    current_paragraph = []
                
                in_code_block = not in_code_block
                if in_code_block:
                    html_lines.append('<pre style="background: #f5f5f5; padding: 15px; border-radius: 5px; overflow-x: auto; margin: 0 0 20px 0;"><code>')
                else:
                    html_lines.append('</code></pre>')
                continue
            
            if in_code_block:
                html_lines.append(escape(line))
                continue
            
            # Handle headers (must be at start of line)
            if line.strip():
                header_match = re.match(r'^(#{1,6})\s+(.*)$', line)
                if header_match:
                    if current_paragraph:
                        html_lines.append('<p style="margin: 0 0 20px 0;">' + ' '.join(current_paragraph) + '</p>')
                        current_paragraph = []
                    
                    level = len(header_match.group(1))
                    text = header_match.group(2)
                    
                    # Process inline formatting in headers
                    text = self._process_inline_formatting(text)
                    
                    if level == 1:
                        html_lines.append(f'<h1 style="margin: 40px 0 30px 0; font-size: 32px; color: #000; font-family: Georgia, serif; font-weight: normal;">{text}</h1>')
                    elif level == 2:
                        html_lines.append(f'<h2 style="margin: 35px 0 20px 0; font-size: 26px; color: #000; font-family: Georgia, serif; font-weight: normal;">{text}</h2>')
                    elif level == 3:
                        html_lines.append(f'<h3 style="margin: 30px 0 15px 0; font-size: 22px; color: #333; font-weight: 600;">{text}</h3>')
                    else:
                        html_lines.append(f'<h{level} style="margin: 25px 0 15px 0; font-size: 18px; color: #333; font-weight: 600;">{text}</h{level}>')
                    continue
                
                # Handle lists
                if line.strip().startswith('- ') or line.strip().startswith('* '):
                    if current_paragraph:
                        html_lines.append('<p style="margin: 0 0 20px 0;">' + ' '.join(current_paragraph) + '</p>')
                        current_paragraph = []
                    
                    # Check if we're continuing a list or starting a new one
                    if not html_lines or not html_lines[-1].endswith('</li>'):
                        html_lines.append('<ul style="margin: 0 0 20px 0; padding-left: 30px;">')
                    
                    list_text = line.strip()[2:]
                    list_text = self._process_inline_formatting(list_text)
                    html_lines.append(f'<li style="margin: 0 0 8px 0; line-height: 1.6;">{list_text}</li>')
                    continue
                elif html_lines and html_lines[-1].endswith('</li>'):
                    # Close the list if we're not continuing
                    html_lines.append('</ul>')
                
                # Handle blockquotes
                if line.strip().startswith('>'):
                    if current_paragraph:
                        html_lines.append('<p style="margin: 0 0 20px 0;">' + ' '.join(current_paragraph) + '</p>')
                        current_paragraph = []
                    
                    quote_text = line.strip()[1:].strip()
                    quote_text = self._process_inline_formatting(quote_text)
                    html_lines.append(f'<blockquote style="margin: 0 0 20px 0; padding-left: 20px; border-left: 4px solid #e0e0e0; color: #666; font-style: italic;">{quote_text}</blockquote>')
                    continue
                
                # Handle horizontal rules
                if re.match(r'^[\-\*_]{3,}$', line.strip()):
                    if current_paragraph:
                        html_lines.append('<p style="margin: 0 0 20px 0;">' + ' '.join(current_paragraph) + '</p>')
                        current_paragraph = []
                    html_lines.append('<hr style="border: none; border-top: 1px solid #e0e0e0; margin: 30px 0;">')
                    continue
                
                # Regular paragraph line
                processed_line = self._process_inline_formatting(line.strip())
                current_paragraph.append(processed_line)
            
            else:
                # Empty line - end current paragraph
                if current_paragraph:
                    html_lines.append('<p style="margin: 0 0 20px 0;">' + ' '.join(current_paragraph) + '</p>')
                    current_paragraph = []
                
                # Close any open lists
                if html_lines and html_lines[-1].endswith('</li>'):
                    html_lines.append('</ul>')
        
        # Don't forget last paragraph
        if current_paragraph:
            html_lines.append('<p style="margin: 0 0 20px 0;">' + ' '.join(current_paragraph) + '</p>')
        
        # Close any open lists at the end
        if html_lines and html_lines[-1].endswith('</li>'):
            html_lines.append('</ul>')
        
        return '\n'.join(html_lines)
    
    def _process_inline_formatting(self, text: str) -> str:
        """Process inline markdown formatting (bold, italic, links, code)"""
        from html import escape
        import re
        
        # Escape HTML first
        text = escape(text)
        
        # Code spans (must be processed before other formatting)
        text = re.sub(r'`([^`]+)`', r'<code style="background: #f5f5f5; padding: 2px 4px; border-radius: 3px; font-size: 0.9em;">\1</code>', text)
        
        # Bold (must be before italic)
        text = re.sub(r'\*\*([^\*]+)\*\*', r'<strong>\1</strong>', text)
        
        # Italic
        text = re.sub(r'\*([^\*]+)\*', r'<em>\1</em>', text)
        
        # Links - with proper URL validation
        def replace_link(match):
            link_text = match.group(1)
            url = match.group(2)
            # Basic URL validation
            if url.startswith(('http://', 'https://', 'mailto:', '/')):
                return f'<a href="{url}" style="color: #0066CC; text-decoration: none;">{link_text}</a>'
            else:
                # Assume relative URL
                return f'<a href="{url}" style="color: #0066CC; text-decoration: none;">{link_text}</a>'
        
        text = re.sub(r'\[([^\]]+)\]\(([^\)]+)\)', replace_link, text)
        
        return text
    
    def create_substack_style_email(self, summaries: List[str], episodes: List[Episode]) -> str:
        """Create clean HTML email with table of contents and better structure"""
        
        # Create table of contents
        toc_html = ""
        for i, episode in enumerate(episodes[:len(summaries)]):
            toc_html += f'''
                <tr>
                    <td style="padding: 8px 0;">
                        <a href="#episode-{i}" style="color: #0066CC; text-decoration: none; font-size: 16px;">
                            <strong>{episode.podcast}</strong>: {episode.title}
                        </a>
                        <div style="font-size: 14px; color: #666; margin-top: 4px;">
                            Release Date: {episode.published.strftime('%B %d, %Y')} • {self._format_duration_display(episode.duration)}
                        </div>
                    </td>
                </tr>'''
        
        # Create episodes HTML with better separation
        episodes_html = ""
        for i, (summary, episode) in enumerate(zip(summaries, episodes[:len(summaries)])):
            # Add anchor for navigation
            episodes_html += f'<tr id="episode-{i}"><td style="padding: 0;">'
            
            if i > 0:
                # Visual separator between episodes
                episodes_html += '''
                    <div style="padding: 60px 0;">
                        <hr style="border: none; border-top: 2px solid #E0E0E0; margin: 0;">
                    </div>'''
            
            # Add title header for this podcast episode
            episodes_html += f'''
                <div style="background: #f0f8ff; padding: 20px; border-radius: 8px; margin-bottom: 30px;">
                    <h2 style="margin: 0 0 10px 0; font-size: 28px; color: #2c3e50; font-family: Georgia, serif; font-weight: normal;">
                        {episode.podcast}
                    </h2>
                    <h3 style="margin: 0 0 15px 0; font-size: 20px; color: #34495e; font-family: Georgia, serif; font-weight: normal;">
                        {episode.title}
                    </h3>
                    <p style="margin: 0; font-size: 14px; color: #666;">
                        Release Date: {episode.published.strftime('%B %d, %Y')} • Duration: {self._format_duration_display(episode.duration)}
                    </p>
                </div>'''
            
            # Convert markdown to HTML
            html_content = self._convert_markdown_to_html(summary)
            
            # Wrap content in a container
            episodes_html += f'''
                <div style="background: #ffffff; padding: 0 0 40px 0;">
                    {html_content}
                    <div style="margin-top: 40px; text-align: right;">
                        <a href="#toc" style="color: #666; text-decoration: none; font-size: 14px;">↑ Back to top</a>
                    </div>
                </div>
            </td></tr>'''
        
        return f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Renaissance Weekly</title>
    <!--[if mso]>
    <style type="text/css">
        table {{border-collapse: collapse;}}
        .outlook-font {{font-family: Arial, sans-serif !important;}}
    </style>
    <![endif]-->
</head>
<body style="margin: 0; padding: 0; font-family: Georgia, serif; font-size: 18px; line-height: 1.6; color: #333; background-color: #FFF; -webkit-text-size-adjust: 100%; -ms-text-size-adjust: 100%;">
    <table role="presentation" cellspacing="0" cellpadding="0" border="0" width="100%" style="background-color: #FFF;">
        <tr>
            <td align="center" style="padding: 40px 20px;">
                <!--[if mso]>
                <table role="presentation" cellspacing="0" cellpadding="0" border="0" width="600">
                <tr>
                <td>
                <![endif]-->
                <table role="presentation" cellspacing="0" cellpadding="0" border="0" width="100%" style="max-width: 600px;">
                    <!-- Header -->
                    <tr>
                        <td style="padding: 0 0 40px 0; text-align: center;">
                            <h1 style="margin: 0 0 10px 0; font-family: Georgia, serif; font-size: 48px; font-weight: normal; letter-spacing: -1px; color: #000; mso-line-height-rule: exactly; line-height: 1.1;">Renaissance Weekly</h1>
                            <p style="margin: 0 0 20px 0; font-size: 18px; color: #666; font-style: italic; font-family: Georgia, serif;">The smartest podcasts, distilled.</p>
                            <p style="margin: 0; font-size: 14px; color: #999; text-transform: uppercase; letter-spacing: 1px; font-family: Arial, sans-serif;">{datetime.now().strftime('%B %d, %Y')}</p>
                        </td>
                    </tr>
                    
                    <!-- Introduction -->
                    <tr>
                        <td style="padding: 0 0 50px 0;">
                            <p style="margin: 0; font-size: 20px; line-height: 1.7; color: #333; font-weight: 300; font-family: Georgia, serif;">In a world of infinite content, attention is the scarcest resource. This week's edition brings you the essential insights from conversations that matter.</p>
                        </td>
                    </tr>
                    
                    <!-- Table of Contents -->
                    <tr id="toc">
                        <td style="padding: 0 0 50px 0;">
                            <div style="background: #f8f8f8; padding: 30px; border-radius: 8px;">
                                <h2 style="margin: 0 0 20px 0; font-size: 24px; color: #000; font-family: Georgia, serif; font-weight: normal;">This Week's Essential Conversations</h2>
                                <table role="presentation" cellspacing="0" cellpadding="0" border="0" width="100%">
                                    {toc_html}
                                </table>
                            </div>
                        </td>
                    </tr>
                    
                    <!-- Episodes -->
                    {episodes_html}
                    
                    <!-- Footer -->
                    <tr>
                        <td style="padding: 60px 0 40px 0; text-align: center; border-top: 1px solid #E0E0E0;">
                            <p style="margin: 0 0 15px 0; font-size: 24px; font-family: Georgia, serif; color: #000;">Renaissance Weekly</p>
                            <p style="margin: 0 0 20px 0; font-size: 16px; color: #666; font-style: italic; font-family: Georgia, serif;">"For those who remain curious."</p>
                            <p style="margin: 0; font-size: 14px; color: #999;">
                                <a href="https://gistcapture.ai" style="color: #666; text-decoration: none;">gistcapture.ai</a>
                            </p>
                        </td>
                    </tr>
                </table>
                <!--[if mso]>
                </td>
                </tr>
                </table>
                <![endif]-->
            </td>
        </tr>
    </table>
</body>
</html>"""
    
    def _create_plain_text_version(self, summaries: List[str]) -> str:
        """Create plain text version"""
        plain = "RENAISSANCE WEEKLY\n"
        plain += "The smartest podcasts, distilled.\n"
        plain += f"{datetime.now().strftime('%B %d, %Y')}\n\n"
        plain += "="*60 + "\n\n"
        
        for summary in summaries:
            # Remove markdown
            text = re.sub(r'\*\*([^*]+)\*\*', r'\1', summary)
            text = re.sub(r'\*([^*]+)\*', r'\1', text)
            text = re.sub(r'\[([^\]]+)\]\(([^\)]+)\)', r'\1 (\2)', text)
            text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)
            
            plain += text + "\n\n" + "="*60 + "\n\n"
        
        plain += "Renaissance Weekly\n"
        plain += "For those who remain curious.\n"
        plain += "https://gistcapture.ai"
        
        return plain


# Podcast configurations - All 19 targeted podcasts with bulletproof sources
PODCAST_CONFIGS = [
    {
        "name": "Market Huddle",
        "description": "Expert market analysis and financial insights",
        "rss_feeds": [
            "https://markethuddle.substack.com/feed",
        ],
        "apple_id": "1552799888",
        "website": "https://markethuddle.substack.com",
        "force_apple": True  # Force Apple as primary source
    },
    {
        "name": "Macro Voices",
        "description": "Global macro investing insights with Erik Townsend",
        "rss_feeds": [
            "https://feeds.feedburner.com/MacroVoices",
        ],
        "apple_id": "1079172742"
    },
    {
        "name": "Forward Guidance",
        "description": "The Fed, macro markets, and investment strategy",
        "rss_feeds": [
            "https://feeds.megaphone.fm/forwardguidance",
        ],
        "apple_id": "1562820083"
    },
    {
        "name": "Odd Lots",
        "description": "Bloomberg's Joe Weisenthal and Tracy Alloway explore markets",
        "rss_feeds": [
            "https://www.omnycontent.com/d/playlist/e73c998e-6e60-432f-8610-ae210140c5b1/8a94442e-5a74-4fa2-8b8d-ae27003a8d6b/982f5071-765c-403d-969d-ae27003a8d83/podcast.rss",
        ],
        "apple_id": "1056200096",
        "force_apple": True
    },
    {
        "name": "BG2 Pod",
        "description": "Bill Gurley on venture capital, technology, and business strategy",
        "rss_feeds": [
            "https://feeds.megaphone.fm/BG2POD",
            "https://feeds.megaphone.fm/BGUR8742038096",
        ],
        "apple_id": "1728994116",
        "force_apple": True  # New podcast, prioritize Apple
    },
    {
        "name": "We Study Billionaires",
        "description": "Investing insights from studying the world's best investors",
        "rss_feeds": [
            "https://feeds.megaphone.fm/PPLLC8974708240",
        ],
        "apple_id": "928933489",
        "force_apple": True
    },
    {
        "name": "American Optimist",
        "description": "Celebrating American innovation and entrepreneurship with Joe Lonsdale",
        "rss_feeds": [
            "https://feeds.transistor.fm/american-optimist-with-joe-lonsdale",
            "https://www.americanoptimist.com/podcast.rss",
        ],
        "apple_id": "1589085277",
        "website": "https://www.americanoptimist.com",
        "force_apple": True  # Force Apple as feeds are problematic
    },
    {
        "name": "All-In",
        "description": "Tech, economics, and politics with Chamath, Jason, Sacks & Friedberg",
        "rss_feeds": [
            "https://feeds.megaphone.fm/all-in-with-chamath-jason-sacks-friedberg",
        ],
        "apple_id": "1502871393",
        "website": "https://www.allinpodcast.co"
    },
    {
        "name": "A16Z",
        "description": "Technology, innovation, and the future from Andreessen Horowitz",
        "rss_feeds": [
            "https://feeds.simplecast.com/JGE3yC0V",
        ],
        "apple_id": "842818711"
    },
    {
        "name": "Lunar Society",
        "description": "Deep conversations about technology and philosophy with Dwarkesh Patel",
        "rss_feeds": [
            "https://www.dwarkeshpatel.com/feed",
            "https://api.dwarkeshpatel.com/feed",
        ],
        "apple_id": "1598388196",
        "website": "https://www.dwarkeshpatel.com"
    },
    {
        "name": "Cognitive Revolution",
        "description": "AI's impact on business and society with Nathan Labenz",
        "rss_feeds": [
            "https://feeds.megaphone.fm/RINTP3108857801",
        ],
        "apple_id": "1669813431",
        "force_apple": True
    },
    {
        "name": "No Priors",
        "description": "AI and tech investing insights with leading VCs",
        "rss_feeds": [
            "https://feeds.megaphone.fm/nopriors",
        ],
        "apple_id": "1663480525"
    },
    {
        "name": "Modern Wisdom",
        "description": "Life lessons and wisdom with Chris Williamson",
        "rss_feeds": [
            "https://modernwisdom.libsyn.com/rss",
        ],
        "apple_id": "1347973549",
        "has_transcripts": True
    },
    {
        "name": "Knowledge Project",
        "description": "Master the best of what other people have figured out with Shane Parrish",
        "rss_feeds": [
            "https://theknowledgeproject.libsyn.com/rss",
        ],
        "apple_id": "990149481",
        "website": "https://fs.blog",
        "has_transcripts": True
    },
    {
        "name": "Founders",
        "description": "Learn from history's greatest entrepreneurs with David Senra",
        "rss_feeds": [
            "https://feeds.redcircle.com/2ff32e90-aaf5-44d9-8a56-1333db3554f8",
        ],
        "apple_id": "1227971746",
        "force_apple": True
    },
    {
        "name": "Tim Ferriss",
        "description": "Life hacks, productivity, and interviews with world-class performers",
        "rss_feeds": [
            "https://rss.art19.com/tim-ferriss-show",
            "https://tim.blog/feed/podcast/",
        ],
        "apple_id": "863897795",
        "website": "https://tim.blog",
        "has_transcripts": True
    },
    {
        "name": "The Drive",
        "description": "Dr. Peter Attia on longevity, health, and performance",
        "rss_feeds": [
            "https://peterattiamd.com/podcast/feed/",
            "https://feeds.megaphone.fm/TDC9352325831",
        ],
        "apple_id": "1227863024",
        "website": "https://peterattiamd.com",
        "has_transcripts": True,
        "force_apple": True
    },
    {
        "name": "Huberman Lab",
        "description": "Science-based tools for everyday life from neuroscientist Andrew Huberman",
        "rss_feeds": [
            "https://feeds.megaphone.fm/hubermanlab",
        ],
        "apple_id": "1545953110",
        "website": "https://hubermanlab.com",
        "has_transcripts": True
    },
    {
        "name": "The Doctor's Farmacy",
        "description": "Dr. Mark Hyman on functional medicine and health optimization",
        "rss_feeds": [
            "https://feeds.megaphone.fm/thedoctorsfarmacy",
            "https://feeds.megaphone.fm/TDC5878721074",
        ],
        "apple_id": "1382804627",
        "force_apple": True
    }
]


def main():
    """Entry point"""
    try:
        # Set up async event loop
        if os.name == 'nt':  # Windows
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        
        # Run the app
        app = RenaissanceWeekly()
        
        # Parse command line arguments
        import sys
        days_back = 7
        mode = "normal"
        podcast_name = None
        
        if len(sys.argv) > 1:
            if sys.argv[1] == "verify":
                mode = "verify"
                if len(sys.argv) > 2 and sys.argv[2].isdigit():
                    days_back = int(sys.argv[2])
            elif sys.argv[1] == "check":
                mode = "check"
                if len(sys.argv) > 2:
                    podcast_name = sys.argv[2]
                    if len(sys.argv) > 3 and sys.argv[3].isdigit():
                        days_back = int(sys.argv[3])
                else:
                    print("Error: Please specify a podcast name")
                    print("Usage: python main.py check \"Podcast Name\" [days]")
                    return
            elif sys.argv[1].isdigit():
                days_back = int(sys.argv[1])
            elif sys.argv[1] in ["-h", "--help"]:
                print("Renaissance Weekly - Podcast Intelligence System\n")
                print("Usage:")
                print("  python main.py [days]                    # Normal mode (default: 7 days)")
                print("  python main.py verify [days]             # Run verification report")
                print("  python main.py check \"Podcast Name\" [days] # Check single podcast")
                print("  python main.py -h                        # Show this help\n")
                print("Environment Variables:")
                print("  VERIFY_APPLE_PODCASTS=true/false   # Enable Apple verification (default: true)")
                print("  FETCH_MISSING_EPISODES=true/false  # Auto-fetch missing episodes (default: true)")
                print("  TESTING_MODE=true/false            # Limit transcription to 20 min (default: true)")
                print("\nExamples:")
                print("  python main.py                     # Process last 7 days")
                print("  python main.py 14                  # Process last 14 days")
                print("  python main.py verify              # Run verification report")
                print("  python main.py check \"Tim Ferriss\" # Debug Tim Ferriss podcast")
                return
        
        # Run appropriate mode
        if mode == "verify":
            asyncio.run(app.run_verification_report(days_back))
        elif mode == "check":
            asyncio.run(app.check_single_podcast(podcast_name, days_back))
        else:
            asyncio.run(app.run(days_back))
        
    except KeyboardInterrupt:
        logger.info("\n⚠️  Interrupted by user")
    except Exception as e:
        logger.error(f"❌ Fatal error: {e}")
        import traceback
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    main()